{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c6d118",
   "metadata": {},
   "source": [
    "# Authors: Nazmi Bunjaku (2775692), Geri Bakushi (2819369)\n",
    "# CIS 492 - Big Data Analytics\n",
    "# Professor Sunnie Chung\n",
    "# Final Project - IMDB Sentiment Analysis\n",
    "# May 05, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de1285",
   "metadata": {},
   "source": [
    "##### Import necessary packages/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d3fca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/besabunjaku/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'IMDB Dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re, string, unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../IMDB Sentiment Analysis/input\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63df7b",
   "metadata": {},
   "source": [
    "### Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b229ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data = pd.read_csv(\"input/IMDB Dataset.csv\")\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1418c7",
   "metadata": {},
   "source": [
    "### Explanatory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e103f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56d85a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370ce774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3868f",
   "metadata": {},
   "source": [
    "### Split the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae254ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (40000,)\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "train_reviews = imdb_data.review[:40000]\n",
    "train_sentiments = imdb_data.sentiment[:40000]\n",
    "\n",
    "# Testing data\n",
    "test_reviews = imdb_data.review[40000:]\n",
    "test_sentiments = imdb_data.sentiment[40000:]\n",
    "\n",
    "print(train_reviews.shape, train_sentiments.shape)\n",
    "print(test_reviews.shape, test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a0900",
   "metadata": {},
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2294406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of text\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# Setting English stopwords\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b36d8",
   "metadata": {},
   "source": [
    "### Removing HTML strips and text noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62739b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        One of the other reviewers has mentioned that ...\n",
      "1        A wonderful little production. The filming tec...\n",
      "2        I thought this was a wonderful way to spend ti...\n",
      "3        Basically there's a family where a little boy ...\n",
      "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
      "                               ...                        \n",
      "49995    I thought this movie did a down right good job...\n",
      "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
      "49997    I am a Catholic taught in parochial elementary...\n",
      "49998    I'm going to have to disagree with the previou...\n",
      "49999    No one expects the Star Trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removal of HTML strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Removing square brackets\n",
    "def remove_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# Removing text noise\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "# Call function on \"review\" column\n",
    "imdb_data['review'] = imdb_data['review'].apply(denoise_text)\n",
    "print(imdb_data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182a453",
   "metadata": {},
   "source": [
    "### Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e980e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        One of the other reviewers has mentioned that ...\n",
      "1        A wonderful little production The filming tech...\n",
      "2        I thought this was a wonderful way to spend ti...\n",
      "3        Basically theres a family where a little boy J...\n",
      "4        Petter Matteis Love in the Time of Money is a ...\n",
      "                               ...                        \n",
      "49995    I thought this movie did a down right good job...\n",
      "49996    Bad plot bad dialogue bad acting idiotic direc...\n",
      "49997    I am a Catholic taught in parochial elementary...\n",
      "49998    Im going to have to disagree with the previous...\n",
      "49999    No one expects the Star Trek movies to be high...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_special_chars(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Apply function on \"review\" column\n",
    "imdb_data['review'] = imdb_data['review'].apply(remove_special_chars)\n",
    "print(imdb_data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65994017",
   "metadata": {},
   "source": [
    "### Text Stemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35df27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "imdb_data['review'] = imdb_data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efe3f7",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23e9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "imdb_data['review'] = imdb_data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250df1f1",
   "metadata": {},
   "source": [
    "### Normalize Training Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a6ab955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train_reviews = imdb_data.review[:40000]\n",
    "norm_train_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04d183",
   "metadata": {},
   "source": [
    "### Normalize Test Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c734e71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read review watch thi piec cinemat garbag took least 2 page find somebodi els didnt think thi appallingli unfunni montag wasnt acm humour 70 inde ani era thi isnt least funni set sketch comedi ive ever seen itll till come along half skit alreadi done infinit better act monti python woodi allen wa say nice piec anim last 90 second highlight thi film would still get close sum mindless drivelridden thi wast 75 minut semin comedi onli world semin realli doe mean semen scatolog humour onli world scat actual fece precursor joke onli mean thi handbook comedi tit bum odd beaver niceif pubesc boy least one hand free havent found playboy exist give break becaus wa earli 70 way sketch comedi go back least ten year prior onli way could even forgiv thi film even made wa gunpoint retro hardli sketch clown subtli pervert children may cut edg circl could actual funni come realli quit sad kept go throughout entir 75 minut sheer belief may save genuin funni skit end gave film 1 becaus wa lower scoreand onli recommend insomniac coma patientsor perhap peopl suffer lockjawtheir jaw would final drop open disbelief'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test_reviews = imdb_data.review[40000:]\n",
    "norm_test_reviews[45005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa16f1",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990c77e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW_cv_TRAIN:  (40000, 6209089)\n",
      "BoW_cv_TEST:  (40000, 6209089)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Count Vectorizer\n",
    "cv = CountVectorizer(min_df=0, max_df=1, binary=False, ngram_range=(1,3))\n",
    "\n",
    "# Transform train reviews\n",
    "cv_train_reviews = cv.fit_transform(norm_train_reviews)\n",
    "\n",
    "# Transform test reviews\n",
    "cv_test_reviews = cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BoW_cv_TRAIN: ', cv_train_reviews.shape)\n",
    "print('BoW_cv_TEST: ', cv_train_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdde57",
   "metadata": {},
   "source": [
    "## Term Frequency-Inverse Document Frequency model (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c6dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF_TRAIN:  (40000, 6209089)\n",
      "TFIDF_TEST:  (10000, 6209089)\n"
     ]
    }
   ],
   "source": [
    "#TFIDF Vectorizer\n",
    "tv = TfidfVectorizer(min_df=0, max_df=1, use_idf=True, ngram_range=(1,3))\n",
    "\n",
    "# Transformed Training Reviews\n",
    "tv_train_reviews = tv.fit_transform(norm_train_reviews)\n",
    "\n",
    "# Transformed Test Reviews\n",
    "tv_test_reviews = tv.transform(norm_test_reviews)\n",
    "\n",
    "print('TFIDF_TRAIN: ', tv_train_reviews.shape)\n",
    "print('TFIDF_TEST: ', tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f7cea",
   "metadata": {},
   "source": [
    "### Labeling the sentiment analysis text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "379e4944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "\n",
    "# Transformed Sentiment Data\n",
    "sentiment_data = lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c806d8",
   "metadata": {},
   "source": [
    "### Split the sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1223ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "train_sentiment_data = sentiment_data[:40000]\n",
    "test_sentiment_data = sentiment_data[40000:]\n",
    "\n",
    "print(train_sentiment_data)\n",
    "print(test_sentiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d0764",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f173cb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n",
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n",
    "\n",
    "# Fitting the model for Bag of Words\n",
    "lr_bow = lr.fit(cv_train_reviews, train_sentiment_data)\n",
    "print(lr_bow)\n",
    "\n",
    "# Fitting the model for TFIDF Features\n",
    "lr_tfidf = lr.fit(tv_train_reviews, train_sentiment_data)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0824d5e",
   "metadata": {},
   "source": [
    "##### Perform Logistic Regression on test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc11f5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n",
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the model for BoW\n",
    "lr_bow_predict = lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "\n",
    "# Predicting the model for TFIDF\n",
    "lr_tfidf_predict = lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c04e53",
   "metadata": {},
   "source": [
    "##### Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38a4400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Bag of Words Accuracy:  0.7512\n",
      "Logistic Regression - TDIDF Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score for the BoW model\n",
    "lr_bow_accuracy = accuracy_score(test_sentiment_data, lr_bow_predict)\n",
    "print('Logistic Regression - Bag of Words Accuracy: ', lr_bow_accuracy)\n",
    "\n",
    "lr_tfidf_accuracy = accuracy_score(test_sentiment_data, lr_tfidf_predict)\n",
    "print('Logistic Regression - TDIDF Accuracy: ', lr_tfidf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03446394",
   "metadata": {},
   "source": [
    "##### Output the Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "303f04a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.75      0.75      0.75      4993\n",
      "    Negative       0.75      0.75      0.75      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.74      0.77      0.75      4993\n",
      "    Negative       0.76      0.73      0.75      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report for BoW\n",
    "lr_bow_report = classification_report(test_sentiment_data, lr_bow_predict, target_names=['Positive', 'Negative'])\n",
    "print(lr_bow_report)\n",
    "\n",
    "# Classification Report for TFIDF\n",
    "lr_tfidf_report = classification_report(test_sentiment_data, lr_tfidf_predict, target_names=['Positive', 'Negative'])\n",
    "print(lr_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59021b",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d26e1710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3768 1239]\n",
      " [1249 3744]]\n",
      "[[3663 1344]\n",
      " [1156 3837]]\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "cm_bow = confusion_matrix(test_sentiment_data, lr_bow_predict, labels=[1,0])\n",
    "print(cm_bow)\n",
    "\n",
    "cm_tfidf = confusion_matrix(test_sentiment_data, lr_tfidf_predict, labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7a31c",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent or Linear Support Vector Machines for BoW and TFIDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "636d7bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(max_iter=500, random_state=42)\n",
      "SGDClassifier(max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Training the linear svm\n",
    "svm = SGDClassifier(loss='hinge', max_iter=500, random_state=42)\n",
    "\n",
    "# Fitting the SVM for BoW\n",
    "svm_bow = svm.fit(cv_train_reviews, train_sentiment_data)\n",
    "print(svm_bow)\n",
    "\n",
    "# Fitting the SVM for TFIDF\n",
    "svm_tfidf = svm.fit(tv_train_reviews, train_sentiment_data)\n",
    "print(svm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958f067",
   "metadata": {},
   "source": [
    "##### Evaluate Model Perfomance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54abe36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM BoW Score:  [1 1 0 ... 1 1 1]\n",
      "SVM TFIDF Score:  [1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the model for BoW\n",
    "svm_bow_predict = svm.predict(cv_test_reviews)\n",
    "print('SVM BoW Score: ', svm_bow_predict)\n",
    "\n",
    "# Predicting the model for TFIDF\n",
    "svm_tfidf_predict = svm.predict(tv_test_reviews)\n",
    "print('SVM TFIDF Score: ', svm_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05273ec",
   "metadata": {},
   "source": [
    "##### Accuracy of the Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a74768f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_bow_score : 0.5829\n",
      "svm_tfidf_score : 0.5112\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "svm_bow_score=accuracy_score(test_sentiment_data, svm_bow_predict)\n",
    "print(\"svm_bow_score :\",svm_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "svm_tfidf_score=accuracy_score(test_sentiment_data, svm_tfidf_predict)\n",
    "print(\"svm_tfidf_score :\",svm_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c421c06d",
   "metadata": {},
   "source": [
    "##### Output the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49aed5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.94      0.18      0.30      4993\n",
      "    Negative       0.55      0.99      0.70      5007\n",
      "\n",
      "    accuracy                           0.58     10000\n",
      "   macro avg       0.74      0.58      0.50     10000\n",
      "weighted avg       0.74      0.58      0.50     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       1.00      0.02      0.04      4993\n",
      "    Negative       0.51      1.00      0.67      5007\n",
      "\n",
      "    accuracy                           0.51     10000\n",
      "   macro avg       0.75      0.51      0.36     10000\n",
      "weighted avg       0.75      0.51      0.36     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "svm_bow_report = classification_report(test_sentiment_data, svm_bow_predict, target_names=['Positive', 'Negative'])\n",
    "print(svm_bow_report)\n",
    "\n",
    "# TFIDF\n",
    "svm_tfidf_report = classification_report(test_sentiment_data, svm_tfidf_predict, target_names=['Positive', 'Negative'])\n",
    "print(svm_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ffef8b",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb4d780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4948   59]\n",
      " [4112  881]]\n",
      "[[5007    0]\n",
      " [4888  105]]\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "cm_bow = confusion_matrix(test_sentiment_data, svm_bow_predict, labels=[1,0])\n",
    "print(cm_bow)\n",
    "\n",
    "# TFIDF\n",
    "cm_tfidf = confusion_matrix(test_sentiment_data, svm_tfidf_predict, labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9de6a",
   "metadata": {},
   "source": [
    "## Multinomial Bayes for BoW & TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06c9e7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "MultinomialNB()\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "mnb = MultinomialNB()\n",
    "# Fitting the SVM for BoW\n",
    "mnb_bow = mnb.fit(cv_train_reviews, train_sentiment_data)\n",
    "print(mnb_bow)\n",
    "# Fitting the SVM for TFIDF\n",
    "mnb_tfidf = mnb.fit(tv_train_reviews, train_sentiment_data)\n",
    "print(mnb_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ba6ef",
   "metadata": {},
   "source": [
    "##### Model Performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03369c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n",
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the model for BoW\n",
    "mnb_bow_predict = mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "\n",
    "# Predicting the model for TFIDF\n",
    "mnb_tfidf_predict = mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8fec25",
   "metadata": {},
   "source": [
    "##### Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39ace969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score:  0.751\n",
      "mnb_tfidf_score:  0.7509\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for MNB BoW\n",
    "mnb_bow_score = accuracy_score(test_sentiment_data, mnb_bow_predict)\n",
    "print('mnb_bow_score: ', mnb_bow_score)\n",
    "\n",
    "# Accuracy for MNB TFIDF\n",
    "mnb_tfidf_score = accuracy_score(test_sentiment_data, mnb_tfidf_predict)\n",
    "print('mnb_tfidf_score: ', mnb_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d92d1",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3722cffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.75      0.76      0.75      4993\n",
      "    Negative       0.75      0.75      0.75      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.75      0.76      0.75      4993\n",
      "    Negative       0.75      0.74      0.75      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "mnb_bow_report = classification_report(test_sentiment_data, mnb_bow_predict, target_names = ['Positive', 'Negative'])\n",
    "print(mnb_bow_report)\n",
    "\n",
    "# TFIDF\n",
    "mnb_tfidf_report = classification_report(test_sentiment_data, mnb_tfidf_predict, target_names = ['Positive', 'Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f9b7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3736 1271]\n",
      " [1219 3774]]\n",
      "[[3729 1278]\n",
      " [1213 3780]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix for BoW\n",
    "cm_bow = confusion_matrix(test_sentiment_data, mnb_bow_predict, labels=[1,0])\n",
    "print(cm_bow)\n",
    "\n",
    "# Confusion Matrix for TFIDF\n",
    "cm_tfidf = confusion_matrix(test_sentiment_data, mnb_tfidf_predict, labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027c516",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52df41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='logistic', hidden_layer_sizes=(3,), random_state=42)\n",
      "MLPClassifier(activation='logistic', hidden_layer_sizes=(3,), random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "ann_clf_bow = MLPClassifier(hidden_layer_sizes=(3,), activation='logistic', solver='adam', max_iter=200, random_state=42)\n",
    "print(ann_clf_bow)\n",
    "\n",
    "ann_clf_tfidf = MLPClassifier(hidden_layer_sizes=(3,), activation='logistic', solver='adam', max_iter=200, random_state=42)\n",
    "print(ann_clf_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21409c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN classifier for BoW\n",
    "ann_clf_bow.fit(cv_train_reviews, train_sentiment_data)\n",
    "\n",
    "# Fitting the ANN classifier for TFIDF\n",
    "ann_clf_tfidf.fit(tv_train_reviews, train_sentiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493fe1a",
   "metadata": {},
   "source": [
    "##### Model Performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the model for BoW\n",
    "ann_bow_predict = ann_clf_bow.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "\n",
    "# Predicting the model for TFIDF\n",
    "ann_tfidf_predict = ann_clf_tfidf.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab30b0",
   "metadata": {},
   "source": [
    "##### Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa43915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for ANN BoW\n",
    "ann_clf_bow_score = accuracy_score(test_sentiment_data, ann_bow_predict)\n",
    "print('ann_bow_score: ', ann_clf_bow_score)\n",
    "\n",
    "# Accuracy for ANN TFIDF\n",
    "ann_clf_tfidf_score = accuracy_score(test_sentiment_data, ann_tfidf_predict)\n",
    "print('ann_tfidf_score: ', ann_clf_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a874e",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fae592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW\n",
    "ann_bow_report = classification_report(test_sentiment_data, ann_bow_predict, target_names = ['Positive', 'Negative'])\n",
    "print(ann_bow_report)\n",
    "\n",
    "# TFIDF\n",
    "ann_tfidf_report = classification_report(test_sentiment_data, ann_tfidf_predict, target_names = ['Positive', 'Negative'])\n",
    "print(ann_tfidf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d092393",
   "metadata": {},
   "source": [
    "## WordCloud for Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for Negative Reviews\n",
    "plt.figure(figsize=(10,10))\n",
    "negative_text = norm_train_reviews[8]\n",
    "word_cloud = WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "negative_words=word_cloud.generate(negative_text)\n",
    "plt.imshow(negative_words,interpolation='bilinear')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27474748",
   "metadata": {},
   "source": [
    "## WordCloud for Positive Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "positive_text = norm_train_reviews[1]\n",
    "word_cloud = WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
    "positive_words=word_cloud.generate(positive_text)\n",
    "plt.imshow(positive_words,interpolation='bilinear')\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
